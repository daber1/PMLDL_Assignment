{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b05dd5",
   "metadata": {},
   "source": [
    "## Downloading a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bcea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4213bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://github.com/skoltech-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip\"\n",
    "r = requests.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bda70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f02616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '1.0-initial-data-exploration.ipynb',\n",
       " '2.0-data-preprocessing.ipynb',\n",
       " '3.0-final-preparation-of-datasets.ipynb',\n",
       " 'filtered.tsv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400e824",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b3e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13cbc2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('filtered.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ada2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that it has extra column (the first one) that we need to remove\n",
    "data.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e7a152",
   "metadata": {},
   "source": [
    "|Column name     |   Description |\n",
    "| --- | --------- |\n",
    "| reference|           original text|\n",
    "|translation|         modified text(less toxic)|\n",
    "|similarity|          cosine similarity of text(how similar they are)|\n",
    "|lenght_diff|         relative length difference($\\frac{\\text{translation}-\\text{ref}}{\\text{ref}}$)|\n",
    "| ref_tox|toxicity of reference|\n",
    "|trn_tox|toxicifiy of translation|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761bc0ca",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0929a",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8b0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def lower_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text_nonum = re.sub(r'\\d+', ' ', text)\n",
    "    return text_nonum\n",
    "\n",
    "def remove_punc(text):\n",
    "    text_nopunc = re.sub(r'[^a-z|\\s]', ' ', text)\n",
    "    return text_nopunc\n",
    "\n",
    "def remove_multi_spaces(text):\n",
    "    text_no_doublespaces = re.sub('\\s+', ' ', text).strip()\n",
    "    return text_no_doublespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dfdd15",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fca2f126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vlad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [w for w in tokens if w not in stop_words]\n",
    "\n",
    "def stem_words(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56cd7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    _lowered = lower_text(text)\n",
    "    _without_numbers = remove_numbers(_lowered)\n",
    "    _without_punct = remove_punc(_without_numbers)\n",
    "    _single_spaced = remove_multi_spaces(_without_punct)\n",
    "    _tokenized = tokenize_text(_single_spaced)\n",
    "    _without_sw = remove_stop_words(_tokenized)\n",
    "    _stemmed = stem_words(_without_sw)\n",
    "    \n",
    "    return _stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b092f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reference'] = data['reference'].apply(preprocess)\n",
    "data['translation'] = data['translation'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754049b3",
   "metadata": {},
   "source": [
    "### Final Data Preporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df16109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_ratio = 0.1\n",
    "train_val, test = train_test_split(\n",
    "    data, test_size=test_ratio, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d1917b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.2\n",
    "train, val = train_test_split(\n",
    "    train_val, test_size=val_ratio, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e4f6e",
   "metadata": {},
   "source": [
    "#### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb2d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def yield_tokens(df):\n",
    "    for _, sample in df.iterrows():\n",
    "        yield list(chain.from_iterable(sample.to_list()[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2943ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a07403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train), specials=special_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bcfa00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brain', 'noth']\n"
     ]
    }
   ],
   "source": [
    "sample = train['reference'][356049]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbd848bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[131, 128]\n"
     ]
    }
   ],
   "source": [
    "encoded = vocab(sample)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e859823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1726d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        ref = torch.tensor(vocab(sample['reference']))\n",
    "        trn = torch.tensor(vocab(sample['translation']))\n",
    "        return ref, trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a283d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    ref_list, trn_list = [], []\n",
    "    for ref, trn in batch:\n",
    "        ref_list.append(torch.tensor(vocab(ref)))\n",
    "        trn_list.append(torch.tensor(vocab(trn)))\n",
    "    padded_refs = pad_sequence(ref_list, batch_first=True, padding_value=0)\n",
    "    padded_trns = pad_sequence(trn_list, batch_first=True, padding_value=0)\n",
    "    return torch.Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d667a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomTextDataset(train, vocab)\n",
    "val_dataset = CustomTextDataset(val, vocab)\n",
    "test_dataset = CustomTextDataset(test, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0159471",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e696299",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab9eda4",
   "metadata": {},
   "source": [
    "<h4> Now, having ready dataloaders we can proceed to developing the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
